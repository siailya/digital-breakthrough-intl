{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7038603,"sourceType":"datasetVersion","datasetId":4049506},{"sourceId":7042407,"sourceType":"datasetVersion","datasetId":4051992}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport torch\n\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,  TensorDataset\nfrom torch.utils.data import Dataset\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import RandomOverSampler\nfrom nltk.tokenize import word_tokenize\n\nimport multiprocessing\n\n\nfrom torchtext import data\nimport os\nimport random \n%matplotlib inline\n\nfrom gensim.test.utils import common_texts\nfrom gensim.corpora.dictionary import Dictionary\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:46:44.940183Z","iopub.execute_input":"2023-11-24T21:46:44.940579Z","iopub.status.idle":"2023-11-24T21:47:02.916048Z","shell.execute_reply.started":"2023-11-24T21:46:44.940530Z","shell.execute_reply":"2023-11-24T21:47:02.914988Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def seed_everything(seed):\n    # Фискирует максимум сидов.\n    # Это понадобится, чтобы сравнение оптимизаторов было корректным\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(123456)\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:02.917930Z","iopub.execute_input":"2023-11-24T21:47:02.918636Z","iopub.status.idle":"2023-11-24T21:47:02.970996Z","shell.execute_reply.started":"2023-11-24T21:47:02.918599Z","shell.execute_reply":"2023-11-24T21:47:02.970162Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#data = pd.read_csv('/Users/kirill/Documents/projects/text_classif/train_dataset_train.csv', sep=';')\ntrain = pd.read_csv('/kaggle/input/classi//train_dataset.csv')\ntest =  pd.read_csv('/kaggle/input/classi/test_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:02.972277Z","iopub.execute_input":"2023-11-24T21:47:02.972664Z","iopub.status.idle":"2023-11-24T21:47:03.237213Z","shell.execute_reply.started":"2023-11-24T21:47:02.972630Z","shell.execute_reply":"2023-11-24T21:47:03.236252Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport string\nimport nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer \nfrom nltk.tokenize import RegexpTokenizer\n\nstopwords = stopwords.words('russian')\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split\n\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop = ['привет','здравствуйте','добрый']\nfor word in stop:\n    stopwords.append(word)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:03.239637Z","iopub.execute_input":"2023-11-24T21:47:03.240005Z","iopub.status.idle":"2023-11-24T21:47:04.232945Z","shell.execute_reply.started":"2023-11-24T21:47:03.239971Z","shell.execute_reply":"2023-11-24T21:47:04.231917Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    table= str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n\n\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stopwords))]\n        corpus.append(words)\n    return corpus\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:04.234313Z","iopub.execute_input":"2023-11-24T21:47:04.235070Z","iopub.status.idle":"2023-11-24T21:47:04.244425Z","shell.execute_reply.started":"2023-11-24T21:47:04.235033Z","shell.execute_reply":"2023-11-24T21:47:04.243532Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dirty_text = train['text']\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:04.245765Z","iopub.execute_input":"2023-11-24T21:47:04.246042Z","iopub.status.idle":"2023-11-24T21:47:04.258937Z","shell.execute_reply.started":"2023-11-24T21:47:04.246010Z","shell.execute_reply":"2023-11-24T21:47:04.257966Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dirty_text = dirty_text.apply(lambda x : remove_html(x))\ndirty_text = dirty_text.apply(lambda x : remove_punct(x))\ndirty_text = dirty_text.apply(lambda x : remove_emoji(x))\ndirty_text = dirty_text.apply(lambda x :remove_URL(x))\ncorpus=create_corpus(dirty_text)\n\n\nfor i in tqdm(range(0, len(corpus))):\n    \n    #corpus[i] = tokenizer.tokenize(\" \".join(corpus[i]))\n    corpus[i] = \" \".join(corpus[i])\n\ntrain[\"clean_text\"] = corpus","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:04.260323Z","iopub.execute_input":"2023-11-24T21:47:04.261150Z","iopub.status.idle":"2023-11-24T21:47:13.936150Z","shell.execute_reply.started":"2023-11-24T21:47:04.261112Z","shell.execute_reply":"2023-11-24T21:47:13.935165Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 18034/18034 [00:08<00:00, 2063.61it/s]\n100%|██████████| 18034/18034 [00:00<00:00, 415050.66it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"dirty_text = test['text']","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:13.937359Z","iopub.execute_input":"2023-11-24T21:47:13.937688Z","iopub.status.idle":"2023-11-24T21:47:13.943829Z","shell.execute_reply.started":"2023-11-24T21:47:13.937660Z","shell.execute_reply":"2023-11-24T21:47:13.942910Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dirty_text = dirty_text.apply(lambda x : remove_html(x))\ndirty_text = dirty_text.apply(lambda x : remove_punct(x))\ndirty_text = dirty_text.apply(lambda x : remove_emoji(x))\ndirty_text = dirty_text.apply(lambda x :remove_URL(x))\ncorpus=create_corpus(dirty_text)\n\n\nfor i in tqdm(range(0, len(corpus))):\n    \n    #corpus[i] = np.array(tokenizer.tokenize(\" \".join(corpus[i])), dtype=str)\n    corpus[i] = \" \".join(corpus[i])\n\ntest[\"clean_text\"] = corpus","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:13.945124Z","iopub.execute_input":"2023-11-24T21:47:13.945459Z","iopub.status.idle":"2023-11-24T21:47:16.317755Z","shell.execute_reply.started":"2023-11-24T21:47:13.945423Z","shell.execute_reply":"2023-11-24T21:47:16.316731Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 4509/4509 [00:02<00:00, 2127.28it/s]\n100%|██████████| 4509/4509 [00:00<00:00, 360038.77it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = train[['clean_text','group_subject']]\ny_train = train['subject']\n\nX_test = test[['clean_text','group_subject']]\ny_test = test['subject']\n\ny = pd.concat([y_train, y_test])\nX = pd.concat([X_train, X_test])\n\ndf = pd.concat([y,X], axis= 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.322144Z","iopub.execute_input":"2023-11-24T21:47:16.322445Z","iopub.status.idle":"2023-11-24T21:47:16.338658Z","shell.execute_reply.started":"2023-11-24T21:47:16.322420Z","shell.execute_reply":"2023-11-24T21:47:16.337668Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ndef Tokenize(column, seq_len):\n    ##Create vocabulary of words from column\n    corpus = [word for text in column for word in text.split()]\n    count_words = Counter(corpus)\n    sorted_words = count_words.most_common()\n    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n\n    ##Tokenize the columns text using the vocabulary\n    text_int = []\n    for text in column:\n        r = [vocab_to_int[word] for word in text.split()]\n        text_int.append(r)\n    \n    ##Add padding to tokens\n    features = np.zeros((len(text_int), seq_len), dtype = int)\n    for i, review in enumerate(text_int):\n        if len(review) <= seq_len:\n            zeros = list(np.zeros(seq_len - len(review)))\n            new = zeros + review\n        else:\n            new = review[: seq_len]\n        features[i, :] = np.array(new)\n        \n    return sorted_words, features\n\n\n\nvocabulary, token_text = Tokenize(pd.concat([train[\"clean_text\"], test[\"clean_text\"]], axis = 0), len((np.max(pd.concat([train[\"clean_text\"], test[\"clean_text\"]], axis = 0))).split()))\nEMBEDDING_DIM = 185","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.339770Z","iopub.execute_input":"2023-11-24T21:47:16.340106Z","iopub.status.idle":"2023-11-24T21:47:16.879588Z","shell.execute_reply.started":"2023-11-24T21:47:16.340077Z","shell.execute_reply":"2023-11-24T21:47:16.878756Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from gensim.utils import tokenize\nfrom gensim.models import FastText, Word2Vec\n\n\nfasttext_train_data = list(map(lambda x: x.split(), X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.test.utils import get_tmpfile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_dictionary = Dictionary(common_texts)\ncommon_corpus = [common_dictionary.doc2bow(text) for text in common_texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_model = Word2Vec(fasttext_train_data, vector_size=EMBEDDING_DIM, epochs = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE = len(vocabulary) + 1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fasttext_model=FastText(vector_size = 185,\n                        window=2,\n                        min_count=2\n                          )\n\nfasttext_model.build_vocab(fasttext_train_data)\nfasttext_model.train(fasttext_train_data, total_examples=len(fasttext_train_data), epochs=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fasttext_model.save('fasttext.mod_2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 185\nembedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n\n\nfor word, token in vocabulary:\n\n    if word in fasttext_model.wv.key_to_index:\n\n        embedding_vector = fasttext_model.wv[word]\n        embedding_matrix[token] = embedding_vector\n\n\nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = token_text\nle = LabelEncoder()\ny = le.fit_transform(df['subject'].values)\nlen(le.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le2 =LabelEncoder()\ny2 = le2.fit_transform(df['group_subject'].values)\nX = np.concatenate((X,y2.reshape(-1,1)), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install imblearn\nfrom imblearn.over_sampling import RandomOverSampler\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ros = RandomOverSampler()\nX_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train, return_counts=True)\nnp.asarray((unique, counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ntest_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\nvalid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \nvalid_loader = DataLoader(valid_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"torch.rand(3).unsqueeze(1).squeeze(1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n                        nn.Conv1d(in_channels, out_channels, padding = 2, kernel_size = 5),\n                        nn.BatchNorm1d(out_channels),\n                        nn.ReLU())\n        self.conv2 = nn.Sequential(\n                        nn.Conv1d(out_channels, out_channels,padding = 2, kernel_size = 5),\n                        nn.BatchNorm1d(out_channels),\n                        nn.ReLU())\n        self.conv3 = nn.Sequential(\n                        nn.Conv1d(out_channels, in_channels, padding = 2,kernel_size = 5),\n                        nn.BatchNorm1d(in_channels),\n                        nn.ReLU())\n        self.relu = nn.ReLU()\n        self.out_channels = out_channels\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = out * residual\n        out = self.relu(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bat = torch.rand(size = (32, 7, 185))\nr= ResidualBlock(7, 7)\nr(bat).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_dim, is_bidirectional):\n        super(Attention, self).__init__()\n        self.is_bidirectional = is_bidirectional\n        # The attention linear layer which transforms the input data to the hidden space\n        self.attn = nn.Linear(hidden_dim * (4 if is_bidirectional else 2), hidden_dim * (2 if is_bidirectional else 1))\n        # The linear layer that calculates the attention scores\n        self.v = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        seq_len = encoder_outputs.size(1)\n        # Concatenate the last two hidden states in case of a bidirectional LSTM\n        if self.is_bidirectional:\n            hidden = torch.cat((hidden[-2], hidden[-1]), dim=-1)\n        else:\n            hidden = hidden[-1]\n        # Repeat the hidden state across the sequence length\n        hidden_repeated = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        # Calculate attention weights\n        attn_weights = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs), dim=2)))\n        # Compute attention scores\n        attn_weights = self.v(attn_weights).squeeze(2)\n        # Apply softmax to get valid probabilities\n        return nn.functional.softmax(attn_weights, dim=1)\n\n\nclass LSTM_Sentiment_Classifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout, is_bidirectional):\n        super(LSTM_Sentiment_Classifier, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = lstm_layers\n        self.is_bidirectional = is_bidirectional\n\n        # The Embedding layer that converts input words to embeddings\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM layer which processes the embeddings\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, lstm_layers, batch_first=True, bidirectional=is_bidirectional)\n        # Attention layer to compute the context vector\n        self.Res_block = ResidualBlock(7, 14)\n        self.attention = Attention(hidden_dim, is_bidirectional)\n        # Fully connected layer which classifies the context vector into classes\n        self.fc = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), num_classes)\n        # Apply LogSoftmax to outputs for numerical stability\n        self.softmax = nn.LogSoftmax(dim=1)\n        # Dropout layer for regularisation\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, hidden):\n        # Transform words to embeddings\n        embedded = self.dropout(self.embedding(x))\n        # Pass embeddings to LSTM\n        embedded = self.Res_block(embedded)\n        out, hidden = self.lstm(embedded, hidden)\n        \n        # Calculate attention weights\n        attn_weights = self.attention(hidden[0], self.dropout(out))\n        # Calculate context vector by taking the weighted sum of LSTM outputs\n        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n        # Classify the context vector\n        out = self.softmax(self.fc(self.dropout(context)))\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        # Factor determines the size of hidden states depending on bidirectionality\n        factor = 2 if self.is_bidirectional else 1\n        # Initial hidden and cell states are zero\n        h0 = torch.zeros(self.num_layers * factor, batch_size, self.hidden_dim).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * factor, batch_size, self.hidden_dim).to(DEVICE)\n        return c0, h0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES =195  #We are dealing with a multiclass classification of 5 classes\nHIDDEN_DIM = 1600#number of neurons of the internal state (internal neural network in the LSTM)\nLSTM_LAYERS = 2#Number of stacked LSTM layers\n\nIS_BIDIRECTIONAL = True # Set this to False for unidirectional LSTM, and True for bidirectional LSTM\n\nLR = 1e-3#Learning rate\nDROPOUT = 0.6#LSTM Dropout\nEPOCHS = 200 #Number of training epoch\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = LSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, LSTM_LAYERS, DROPOUT, IS_BIDIRECTIONAL)\n\nmodel = model.to(DEVICE)\n\n# Initialize the embedding layer with the previously defined embedding matrix\nmodel.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n# Allow the embedding matrix to be fine-tuned to better adapt to our dataset and get higher accuracy\nmodel.embedding.weight.requires_grad = True\n\n# Set up the criterion (loss function)\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_step = len(train_loader)\ntotal_step_val = len(valid_loader)\n\nearly_stopping_patience = 16\nearly_stopping_counter = 0\n\nvalid_acc_max = 0 # Initialize best accuracy top 0\ntorch.autograd.set_detect_anomaly(True)\nfor e in range(EPOCHS):\n\n    #lists to host the train and validation losses of every batch for each epoch\n    train_loss, valid_loss  = [], []\n    #lists to host the train and validation accuracy of every batch for each epoch\n    train_acc, valid_acc  = [], []\n\n    #lists to host the train and validation predictions of every batch for each epoch\n    y_train_list, y_val_list = [], []\n\n    #initalize number of total and correctly classified texts during training and validation\n    correct, correct_val = 0, 0\n    total, total_val = 0, 0\n    running_loss, running_loss_val = 0, 0\n\n\n    ####TRAINING LOOP####\n\n    model.train()\n\n    for inputs, labels in tqdm(train_loader):\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n        h = model.init_hidden(labels.size(0))\n\n        model.zero_grad() #reset gradients \n\n        output, h = model(inputs,h) #get output and hidden states from LSTM network\n \n        loss = criterion(output, labels)\n        loss.backward()\n        \n        running_loss += loss.item()\n        \n        optimizer.step()\n\n        y_pred_train = torch.argmax(output, dim=1)#get tensor of predicted values on the training set\n        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n\n        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n        total += labels.size(0) #count total texts per batch\n        #correct += f1_score(labels.cpu(), y_train_list, average = 'weighted')\n\n    train_loss.append(running_loss / total_step)\n    train_acc.append(100* correct/total)\n\n    ####VALIDATION LOOP####\n    print('Validation')\n    with torch.no_grad():\n        \n        model.eval()\n        \n        for inputs, labels in tqdm(valid_loader):\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n\n            val_h = model.init_hidden(labels.size(0))\n\n            output, val_h = model(inputs, val_h)\n\n            val_loss = criterion(output, labels)\n            #scheduler.step(val_loss)\n            running_loss_val += val_loss.item()\n\n            y_pred_val = torch.argmax(output, dim=1)\n            y_val_list.extend(y_pred_val.squeeze().tolist())\n\n            correct_val += torch.sum(y_pred_val==labels).item()\n            total_val += labels.size(0)\n\n        valid_loss.append(running_loss_val / total_step_val)\n        scheduler.step(valid_loss[-1])\n        valid_acc.append(100 * correct_val / total_val)\n\n    #Save model if validation accuracy increases\n    if np.mean(valid_acc) >= valid_acc_max:\n        torch.save(model.state_dict(), './state_dict.pt')\n        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n        valid_acc_max = np.mean(valid_acc)\n        early_stopping_counter=0 #reset counter if validation accuracy increases\n    else:\n        print(f'Epoch {e+1}:Validation accuracy did not increase')\n        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n        \n    if early_stopping_counter > early_stopping_patience:\n        print('Early stopped at epoch :', e+1)\n        break\n    \n    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, test_loader):\n    model.eval()\n    y_pred_list = []\n    y_test_list = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            test_h = model.init_hidden(labels.size(0))\n\n            output, val_h = model(inputs, test_h)\n            y_pred_test = torch.argmax(output, dim=1)\n            y_pred_list.extend(y_pred_test.squeeze().tolist())\n            y_test_list.extend(labels.squeeze().tolist())\n    \n    return y_pred_list, y_test_list\n\ny_pred_list, y_test_list = evaluate_model(model, test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, test_loader):\n    model.eval()\n    y_pred_list = []\n    y_test_list = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            test_h = model.init_hidden(labels.size(0))\n\n            output, val_h = model(inputs, test_h)\n            y_pred_test = torch.argmax(output, dim=1)\n            y_pred_list.extend(y_pred_test.squeeze().tolist())\n            y_test_list.extend(labels.squeeze().tolist())\n    \n    return y_pred_list, y_test_list\n\ny_pred_list, y_test_list = evaluate_model(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(y_true, y_pred, average='weighted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\ndf_cm = pd.DataFrame(confusion_matrix(y_test_list,y_pred_list), range(len(le.classes_)), range(len(le.classes_)))\nplt.figure(figsize=(10,7))\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BERT","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"le_gr = LabelEncoder()\ntrain['group_subject'] = le_gr.fit_transform(train['group_subject'])\ntest['group_subject'] = le_gr.transform(test['group_subject'])\n\nle_ex = LabelEncoder()\ntrain['executor'] = le_ex.fit_transform(train['executor'])\ntest['executor'] = le_ex.transform(test['executor'])","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.880823Z","iopub.execute_input":"2023-11-24T21:47:16.881170Z","iopub.status.idle":"2023-11-24T21:47:16.902957Z","shell.execute_reply.started":"2023-11-24T21:47:16.881137Z","shell.execute_reply":"2023-11-24T21:47:16.902013Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = train[['clean_text','group_subject', 'executor']]\n#X_train = train['clean_text']\ny_train = train['subject']\nX_test = test[['clean_text','group_subject', 'executor']]\n#X_test = test['clean_text']\ny_test = test['subject']\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)\ny = np.concatenate([y_train, y_test])\nX = pd.concat([X_train, X_test])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.904106Z","iopub.execute_input":"2023-11-24T21:47:16.904363Z","iopub.status.idle":"2023-11-24T21:47:16.922402Z","shell.execute_reply.started":"2023-11-24T21:47:16.904340Z","shell.execute_reply":"2023-11-24T21:47:16.921453Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.923804Z","iopub.execute_input":"2023-11-24T21:47:16.924325Z","iopub.status.idle":"2023-11-24T21:47:16.970328Z","shell.execute_reply.started":"2023-11-24T21:47:16.924297Z","shell.execute_reply":"2023-11-24T21:47:16.969243Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\nmodel = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:16.971612Z","iopub.execute_input":"2023-11-24T21:47:16.971914Z","iopub.status.idle":"2023-11-24T21:47:20.057492Z","shell.execute_reply.started":"2023-11-24T21:47:16.971887Z","shell.execute_reply":"2023-11-24T21:47:20.056609Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def bert_tokenizer(data):\n    input_ids = []\n    attention_masks = []\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=sent,\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n            max_length= 512,             # Choose max length to truncate/pad\n            pad_to_max_length=True,         # Pad sentence to max length \n            return_attention_mask=True,\n            truncation = True# Return attention mask\n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:20.058618Z","iopub.execute_input":"2023-11-24T21:47:20.058903Z","iopub.status.idle":"2023-11-24T21:47:20.065899Z","shell.execute_reply.started":"2023-11-24T21:47:20.058876Z","shell.execute_reply":"2023-11-24T21:47:20.064751Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train['clean_text']]\n\n# Find the longest tokenized tweet\nmax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max length: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:20.067071Z","iopub.execute_input":"2023-11-24T21:47:20.067363Z","iopub.status.idle":"2023-11-24T21:47:22.727001Z","shell.execute_reply.started":"2023-11-24T21:47:20.067338Z","shell.execute_reply":"2023-11-24T21:47:22.726065Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Max length:  709\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain_inputs, train_masks = bert_tokenizer(X_train['clean_text'])\nval_inputs, val_masks = bert_tokenizer(X_valid['clean_text'])\ntest_inputs, test_masks = bert_tokenizer(X_test['clean_text'])","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:22.728224Z","iopub.execute_input":"2023-11-24T21:47:22.728556Z","iopub.status.idle":"2023-11-24T21:47:37.888721Z","shell.execute_reply.started":"2023-11-24T21:47:22.728529Z","shell.execute_reply":"2023-11-24T21:47:37.887612Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 738","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.889999Z","iopub.execute_input":"2023-11-24T21:47:37.890299Z","iopub.status.idle":"2023-11-24T21:47:37.894567Z","shell.execute_reply.started":"2023-11-24T21:47:37.890272Z","shell.execute_reply":"2023-11-24T21:47:37.893568Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_labels = torch.from_numpy(y_train)\nval_labels = torch.from_numpy(y_valid)\ntest_labels = torch.from_numpy(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.896072Z","iopub.execute_input":"2023-11-24T21:47:37.896460Z","iopub.status.idle":"2023-11-24T21:47:37.906540Z","shell.execute_reply.started":"2023-11-24T21:47:37.896427Z","shell.execute_reply":"2023-11-24T21:47:37.905640Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"batch_size = 10","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.907821Z","iopub.execute_input":"2023-11-24T21:47:37.908477Z","iopub.status.idle":"2023-11-24T21:47:37.916998Z","shell.execute_reply.started":"2023-11-24T21:47:37.908449Z","shell.execute_reply":"2023-11-24T21:47:37.916065Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.918219Z","iopub.execute_input":"2023-11-24T21:47:37.918593Z","iopub.status.idle":"2023-11-24T21:47:37.927158Z","shell.execute_reply.started":"2023-11-24T21:47:37.918556Z","shell.execute_reply":"2023-11-24T21:47:37.926395Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(train_inputs, train_masks, train_labels, torch.from_numpy(X_train[['executor', 'group_subject']].values))\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels, torch.from_numpy(X_valid[['executor', 'group_subject']].values))\nval_dataloader = DataLoader(val_data, batch_size=batch_size)\n\n# Create the DataLoader for our test set\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, torch.from_numpy(X_test[['executor', 'group_subject']].values))\ntest_dataloader = DataLoader(test_data,batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.928354Z","iopub.execute_input":"2023-11-24T21:47:37.928746Z","iopub.status.idle":"2023-11-24T21:47:37.940918Z","shell.execute_reply.started":"2023-11-24T21:47:37.928720Z","shell.execute_reply":"2023-11-24T21:47:37.939856Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Bert_Classifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(Bert_Classifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n        n_input = 768\n        n_hidden = 128\n        n_output = 195\n\n        # Instantiate BERT model\n        self.bert = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n\n        # Instantiate the classifier (a fully connected layer followed by a ReLU activation and another fully connected layer)\n        self.classifier = nn.Sequential(\n            nn.Linear(n_input+2, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output)\n        )\n\n        # Freeze the BERT model weights if freeze_bert is True (useful for feature extraction without fine-tuning)\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask, metka):\n        # Feed input data (input_ids and attention_mask) to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the `[CLS]` token from the BERT output (useful for classification tasks)\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        \n        # Feed the extracted hidden state to the classifier to compute logits\n        logits = self.classifier(torch.cat((last_hidden_state_cls, metka), axis = 1))\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.942067Z","iopub.execute_input":"2023-11-24T21:47:37.942341Z","iopub.status.idle":"2023-11-24T21:47:37.951657Z","shell.execute_reply.started":"2023-11-24T21:47:37.942317Z","shell.execute_reply":"2023-11-24T21:47:37.950612Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def initialize_model(epochs=4):\n    # Instantiate Bert Classifier\n    bert_classifier = Bert_Classifier(freeze_bert=False)\n\n    bert_classifier.to(device)\n\n    # Set up optimizer\n    optimizer = torch.optim.AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # learning rate, set to default value\n                      eps=1e-8    # decay, set to default value\n                      )\n\n    # Calculate total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Define the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.952965Z","iopub.execute_input":"2023-11-24T21:47:37.953679Z","iopub.status.idle":"2023-11-24T21:47:37.964824Z","shell.execute_reply.started":"2023-11-24T21:47:37.953652Z","shell.execute_reply":"2023-11-24T21:47:37.963961Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nEPOCHS=2","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.968759Z","iopub.execute_input":"2023-11-24T21:47:37.969302Z","iopub.status.idle":"2023-11-24T21:47:37.980000Z","shell.execute_reply.started":"2023-11-24T21:47:37.969276Z","shell.execute_reply":"2023-11-24T21:47:37.979181Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:37.981113Z","iopub.execute_input":"2023-11-24T21:47:37.981505Z","iopub.status.idle":"2023-11-24T21:47:41.880255Z","shell.execute_reply.started":"2023-11-24T21:47:37.981478Z","shell.execute_reply":"2023-11-24T21:47:41.879440Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:41.881509Z","iopub.execute_input":"2023-11-24T21:47:41.881842Z","iopub.status.idle":"2023-11-24T21:47:41.886161Z","shell.execute_reply.started":"2023-11-24T21:47:41.881816Z","shell.execute_reply":"2023-11-24T21:47:41.885250Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Define Cross entropy Loss function for the multiclass classification task\nloss_fn = nn.CrossEntropyLoss()\n\ndef bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        print(\"-\"*10)\n        print(\"Epoch : {}\".format(epoch_i+1))\n        print(\"-\"*10)\n        print(\"-\"*38)\n        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n        print(\"-\"*38)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n        \n        ###TRAINING###\n\n        # Put the model into the training mode\n        model.train()\n\n        for step, batch in tqdm(enumerate(train_dataloader)):\n            batch_counts +=1\n            \n            b_input_ids, b_attn_mask, b_labels, b_metka = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n            \n            # Perform a forward pass and get logits.\n            logits = model(b_input_ids, b_attn_mask, b_metka)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update model parameters:\n            # fine tune BERT params and train additional dense layers\n            optimizer.step()\n            # update learning rate\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 100 batches\n            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n                \n                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        ###EVALUATION###\n        \n        # Put the model into the evaluation mode\n        model.eval()\n        \n        # Define empty lists to host accuracy and validation for each batch\n        val_accuracy = []\n        val_loss = []\n\n        for batch in tqdm(val_dataloader):\n            batch_input_ids, batch_attention_mask, batch_labels, batch_metka = tuple(t.to(device) for t in batch)\n            \n            # We do not want to update the params during the evaluation,\n            # So we specify that we dont want to compute the gradients of the tensors\n            # by calling the torch.no_grad() method\n            with torch.no_grad():\n                logits = model(batch_input_ids, batch_attention_mask, batch_metka)\n\n            loss = loss_fn(logits, batch_labels)\n\n            val_loss.append(loss.item())\n\n            # Get the predictions starting from the logits (get index of highest logit)\n            preds = torch.argmax(logits, dim=1).flatten()\n\n            # Calculate the validation accuracy \n            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n            val_accuracy.append(accuracy)\n\n        # Compute the average accuracy and loss over the validation set\n        val_loss = np.mean(val_loss)\n        val_accuracy = np.mean(val_accuracy)\n        \n        # Print performance over the entire training data\n        time_elapsed = time.time() - t0_epoch\n        print(\"-\"*61)\n        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n        print(\"-\"*61)\n        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n        print(\"-\"*61)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:41.887374Z","iopub.execute_input":"2023-11-24T21:47:41.887694Z","iopub.status.idle":"2023-11-24T21:47:41.906461Z","shell.execute_reply.started":"2023-11-24T21:47:41.887669Z","shell.execute_reply":"2023-11-24T21:47:41.905375Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T21:47:41.907673Z","iopub.execute_input":"2023-11-24T21:47:41.907955Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start training...\n\n----------\nEpoch : 1\n----------\n--------------------------------------\nBATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n--------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"101it [00:57,  1.77it/s]","output_type":"stream"},{"name":"stdout","text":"   100    |   4.837880   |   57.43  \n","output_type":"stream"},{"name":"stderr","text":"201it [01:53,  1.77it/s]","output_type":"stream"},{"name":"stdout","text":"   200    |   4.179005   |   56.44  \n","output_type":"stream"},{"name":"stderr","text":"301it [02:50,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   300    |   3.913521   |   57.12  \n","output_type":"stream"},{"name":"stderr","text":"401it [03:48,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   400    |   3.666149   |   57.24  \n","output_type":"stream"},{"name":"stderr","text":"501it [04:45,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   500    |   3.471567   |   57.26  \n","output_type":"stream"},{"name":"stderr","text":"601it [05:42,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"   600    |   3.358772   |   57.32  \n","output_type":"stream"},{"name":"stderr","text":"701it [06:40,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   700    |   3.130322   |   57.26  \n","output_type":"stream"},{"name":"stderr","text":"801it [07:37,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   800    |   3.099116   |   57.27  \n","output_type":"stream"},{"name":"stderr","text":"901it [08:34,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   900    |   3.004574   |   57.26  \n","output_type":"stream"},{"name":"stderr","text":"1001it [09:31,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"  1000    |   2.869706   |   57.31  \n","output_type":"stream"},{"name":"stderr","text":"1101it [10:29,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"  1100    |   2.833166   |   57.25  \n","output_type":"stream"},{"name":"stderr","text":"1201it [11:26,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"  1200    |   2.769409   |   57.29  \n","output_type":"stream"},{"name":"stderr","text":"1301it [12:23,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"  1300    |   2.731527   |   57.31  \n","output_type":"stream"},{"name":"stderr","text":"1401it [13:21,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"  1400    |   2.718738   |   57.26  \n","output_type":"stream"},{"name":"stderr","text":"1443it [13:44,  1.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"  1442    |   2.660921   |   23.90  \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 361/361 [01:05<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------------------------\nAVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n-------------------------------------------------------------\n   3.309038    |  2.591460  |       42.79       |  890.66  \n-------------------------------------------------------------\n\n\n----------\nEpoch : 2\n----------\n--------------------------------------\nBATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n--------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"101it [00:57,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   100    |   2.593932   |   57.73  \n","output_type":"stream"},{"name":"stderr","text":"201it [01:55,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"   200    |   2.424460   |   57.30  \n","output_type":"stream"},{"name":"stderr","text":"301it [02:52,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"   300    |   2.484624   |   57.35  \n","output_type":"stream"},{"name":"stderr","text":"401it [03:49,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"   400    |   2.450799   |   57.32  \n","output_type":"stream"},{"name":"stderr","text":"501it [04:46,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"   500    |   2.284295   |   57.29  \n","output_type":"stream"},{"name":"stderr","text":"539it [05:08,  1.75it/s]","output_type":"stream"}]},{"cell_type":"code","source":"a = torch.rand([10, 2])\nb = torch.rand([10, 768])\n\nc = torch.cat((a,b), axis = 1)\nc.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_predict(model, test_dataloader):\n    \n    # Define empty list to host the predictions\n    preds_list = []\n    \n    # Put the model into evaluation mode\n    model.eval()\n    \n    for batch in test_dataloader:\n        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n        \n        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n        with torch.no_grad():\n            logit = model(batch_input_ids, batch_attention_mask)\n        \n        # Get index of highest logit\n        pred = torch.argmax(logit,dim=1).cpu().numpy()\n        # Append predicted class to list\n        preds_list.extend(pred)\n\n    return preds_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preds = bert_predict(bert_classifier, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
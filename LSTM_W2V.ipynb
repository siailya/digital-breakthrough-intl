{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_dataset_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kirill/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-11-23 20:20:08.966311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stopwords = stopwords.words('russian')\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop = ['привет','здравствуйте','добрый']\n",
    "for word in stop:\n",
    "    stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table= str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(data['Текст инцидента']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stopwords))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23128/23128 [00:14<00:00, 1550.96it/s]\n",
      "100%|██████████| 23128/23128 [00:00<00:00, 499538.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dirty_text = dirty_text.apply(lambda x : remove_html(x))\n",
    "dirty_text = dirty_text.apply(lambda x : remove_punct(x))\n",
    "dirty_text = dirty_text.apply(lambda x : remove_emoji(x))\n",
    "dirty_text = dirty_text.apply(lambda x :remove_URL(x))\n",
    "corpus=create_corpus(dirty_text)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(corpus))):\n",
    "    \n",
    "    corpus[i] = \" \".join(corpus[i])\n",
    "\n",
    "data[\"clean_text\"] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['clean_text']\n",
    "y = data['Исполнитель']\n",
    "\n",
    "df = data[['Исполнитель', 'clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text, tokenizer):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input text with the configured padding and truncation. Then,\n",
    "    returns the input dictionary, which contains the following keys: \"input_ids\",\n",
    "    \"token_type_ids\" and \"attention_mask\". Each value is a torch.tensor.\n",
    "    :param cfg: configuration class with a TOKENIZER attribute.\n",
    "    :param text: a numpy array where each value is a text as string.\n",
    "    :return inputs: python dictionary where values are torch tensors.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=128,\n",
    "        padding='max_length', # TODO: check padding to max sequence in batch\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def collate(inputs):\n",
    "    \"\"\"\n",
    "    It truncates the inputs to the maximum sequence length in the batch. \n",
    "    \"\"\"\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max()) # Get batch's max sequence length\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df['clean_text'].values\n",
    "        self.labels = df['Исполнитель'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = {}\n",
    "        output[\"inputs\"] = prepare_input(self.texts[item], self.tokenizer)\n",
    "        output[\"labels\"] = torch.tensor(self.labels[item], dtype=torch.float) # TODO: check dtypes\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "X = df.loc[:, df.columns != \"Исполнитель\"]\n",
    "y = df.loc[:, df.columns == \"Исполнитель\"]\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "    df.loc[valid_index, \"fold\"] = i\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "train_folds = df[df['fold'] != fold].reset_index(drop=True)\n",
    "valid_folds = df[df['fold'] == fold].reset_index(drop=True)\n",
    "valid_labels = valid_folds['Исполнитель'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== DATASETS ==========\n",
    "train_dataset = CustomDataset(train_folds, tokenizer)\n",
    "valid_dataset = CustomDataset(valid_folds, tokenizer)\n",
    "\n",
    "# ======== DATALOADERS ==========\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size= 32, # TODO: split into train and valid\n",
    "                            shuffle=True,\n",
    "                            num_workers= multiprocessing.cpu_count(), pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            num_workers=multiprocessing.cpu_count(), pin_memory=True, drop_last=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
